{"name":"DEVICE AWARE 3D LIGHT PAINTING","tagline":"","body":"This report is submitted as part requirement for the BSc Degree in Computer Science at UCL. It is substantially the result of my own work except where explicitly indicated in the text.\r\n \r\n                                    Abstract\r\n   The project was to develop an android application for light painting photography. The application extrudes 3D models, while taking a picture in a long exposure. This will result in a picture of 3D objects hovering in the air creating futuristic effect. The first part was recreating few similar existing applications. The second part of the project is research based, exploring possibility of using phone's sensors for sensing device motion and incorporating this knowledge to the application. It particularly focuses on two Android sensors: rotation vector  and linear acceleration. The research part describes many choices made during the development of the object including rotation representation: Euler angles vs Quaternions, accelerometer choice: acceleration sensor vs linear acceleration sensor, etc. It also summarises some of the relevant analysis made in this field like accelerometer noise and filtering. The research part of the project was challenging as phone accelerometer sensor was never used before for finding position of the device. \r\n   The project achieved developed light painting application with rotation sensing capabilities and also made substantial research on the double integration of the acceleration obtained from the MEMS sensors incorporated in the smart phones today. \r\n___\r\n                                    Table of contents\r\n\r\n1. Introduction\r\n        1.1. Background information\r\n        1.2. Problem statement \r\n1.3.Project's aim and goals\r\n1.4. Organization of the report\r\n2. Related work\r\n2.1. Outline of the sources of information and related work\r\n2.2 Technologies Used\r\n3. Design and implementation\r\n3.1.Loading a model into the application and fitting it into the screen\r\n3.2. Light painting iteration 1. Slicing 3D geometry\r\n3.3. Camera settings for light painting\r\n3.4. ImageAverager or if you are too poor for a DSLR\r\n3.5. Light painting iteration 2. Device-aware rotation of the geometry \r\n3.6. Light painting iteration 3. Device-aware extrusion of the geometry\r\n3.7. GraphHelper\r\n4. Digital Signal Processing or What I would do if I had more time\r\n5. Conclusion and evaluation\r\n5.1. Future work\r\n5.2. Evaluation \r\n___\r\n\r\n                                    Chapter 1. Introduction\r\n\r\n**1.1 Background information** \r\n  Light painting is an old photography technique of taking photo of a light traces in dark location using long exposure and a light source such as a torch as a real-world paint brush. The technique was developed for research purposes as a part of a national program of psychic and physical regeneration and military preparedness and was used to study movements of human body, especially gymnasts’ and soldiers’.[1] The first known such light painting photography is called “Pathological walk from in front” and was taken by attaching incandescent bulbs to the joints of a person. As many other military inventions, light painting has been adapted for more peaceful purposes by ordinary photographers as an artistic endeavor.  \r\n  With emergence of smart phones and mass development of applications the idea migrated to the app markets of Android phones, IPhones  and IPads. Most of them use the device as a simple torch, i.e. a circle in different colors and sizes or to draw holographic 2D or 3D texts.\r\n  The most advanced of such applications is “Holographium” - currently available in app store. It has many features such as importing logos, icons and photos in PNG, BMP, JPG, GIF formats  and converting them to 3D objects. It also has functions of user definable extrusion time and depth. However the idea of my application came from “Making Future Magic” of Dentsu London studio.\r\n\r\n**1.2.Problem statement**\r\n  In order to take good light painting photo it is important to balance between the exposure time, the speed of the extrusion and screen angle. That’s why it normally takes many attempts to get it right, mostly photos come out squashed or blurred and with intrusive haziness.  While exposure time depends on the settings of a camera, the extrusion speed and the screen angle depends on the person dragging the phone. It has to be possible to adjust the 3D model being extruded according to the movement and rotation  of the device and make extrusion process independent of the human factor.\r\n\r\n**1.3.Project's aim and goals**\r\n  My project’s aim is to take advantage of computer graphics techniques and newly emerging area of Android programming - sensor programming and take the good old light painting to the next level. The latter, use of phone sensors, is what makes my application different from the available applications I’ve described and what makes it both challenging and interesting project. \r\n  In order to carry out the project, first I had to learn many technologies such as android programming, especially its Opengl ES API(Opengl for Embedded Systems) for displaying high-end, animated graphics and Android sensor programming which requires understanding what are the physical values the sensors measure, what are the physical intuition behind them and how to interpret them.  As well as these programming interfaces, it was also necessary to inquire theoretical knowledge and computer graphics concepts such as camera position, bounding box, projection, transformation matrices, rotation mathematics such as Euler angles and Quaternions. Of course, I didn’t learn everything, but in order to gain the necessary knowledge I had to cover much wider range than it was necessary.\r\n  I’ve carried out my project in incremental and iterative fashion, developing one goal at each iteration, with each iteration developing on top of existing functionalities and as such enhancing the application capabilities. \r\n \r\nThese goals were set in the beginning:  \r\n1.  Parse a model object from an .obj file and load it into the screen  \r\n2.  Make it possible to load any object so that the object’s centre is in the centre of the phone screen and the object fits into the screen  \r\n3. Move small virtual window along the model in order to get slices or cross-sections of a model, similar to CAT scan used in medical imaging. These slices are rendered in the application so that later, while dragging the phone in front of the camera and taking photo in a long exposure, the slices are extruded and the composition of these 2D slices form 3D image in the photo. This is very similar to stop motion, where hundreds of photos of still objects in different positions form illusion of a movement.  \r\n4. Adding some functionalities to the user interface, like buttons to increase and decrease near and far planes of the window, i.e. specify the “thickness”of the 3D painting.  \r\nTo make it independent of the human factor it is possible to sense the\r\ndevice in the environment and adjust the 3D model accordingly. Therefore following goals were set as well:  \r\n5. Find the appropriate sensor to sense how much was the phone rotated, i.e. what is the angle of rotation of the phone and use this information to keep the 3D model stable and stay in its original orientation, independent from the rotation of a phone.   \r\n6. Use the appropriate sensor to extrude slices as quick as the phone is dragged.  \r\nGoals number 5 and 6 are where the capabilities of Android sensors were to be explored and used to find the rotational angle and speed of the device movement. This is the research part of the project, which aims to see if it is possible, are the sensors accurate enough and will they fit into purpose.  \r\n  The report is organised in sequential fashion, building up the application as it advances. \r\n\r\n**1.4 Organization of the report**  \r\n\r\n_Chapter 2. Related work_  \r\n  In this chapter I will outline and reference the sources of information: research papers, source codes and books which assisted my project. I will also list software tools I used during the development.  \r\n_Chapter 3. Design and implementation_  \r\n  This chapter is the core of the report where I'll describe the development process of each iteration and show deliverables of each iteration. I'll also go through little helper applications which I've built to assist my research.  \r\n_Chapter 4. Digital Signal Processing or What I would do if I had more time_  \r\n  This chapter talks about types of noise in the accelerometer sensor that constrains realisation of some ideas. And also describes different filtering techniques that might be used to eliminate these noise.  \r\n_Chapter 5. Conclusion and evaluation._  \r\n  This is capstone chapter where I will sum up the project, its derivations and lessons learned from it and will provide evaluation of achievements of goals,as well as future work ideas.  \r\n\r\n                                    Chapter 2. Related work\r\n**2.1. Outline of the sources of information and  related work**  \r\n+  Android’s developer site is the primary place where I learned programming Android applications\r\n“OpenGL ES 2.0 Programming guide” by Aaftab Munshi, Dan Ginsburg and Dave Shreiner is where I learned basics of OpenGL ES.  \r\n+  “Android Sensor Programming” by Greg Milette and Adam Stroud was of great help. Part 2 of this book, “Inferring information from physical sensors”, gave me extensive base necessary to deal with sensor programming of my project.  \r\n+  “Implementing Positioning Algorithms Using Accelerometers” - report paper by Kurt Seifert and Oscar Camacho describes a positioning algorithm using the MMA7260QT 3-Axis accelerometer with adjustable sensitivity from ±1.5 g to ±6 g. Although this industrial accelerometer sensor is of better quality in terms of noise and sensitivity compared to micro-machined electromechanical systems (MEMS) accelerometer sensors(usually with maximum range 2g) used in current cell phones, it gives ground to build on.  \r\n+  “An introduction to inertial navigation”  - a technical report by Oliver J.Woodman, researches the error characteristics of inertial systems. It focuses on strapdown systems based on MEMS devices and particularly useful - describes MEMS accelerometers’ error characteristics such as constant bias, velocity random walk, bias stability, temperature effects and calibration errors.\r\n\r\n**2.2 Technologies Used**\r\n    This is a brief summary of technologies used in the project, how and where they were used.  \r\n+ Android SDK(Software Development Kit) provides the API libraries and developer tools necessary to build, test, and debug apps for Android including Eclipse IDE and Android Development Tools.  \r\n+ Eclipse IDE  is a software development environment with extensible plug-in system for customising its base workspace.  \r\n+ ADT (Android Development Tools) is a plugin for Eclipse that provides a suite of Android specific tools for project creation, building, packaging, installation, debugging as well as Java programming language and XML editors and importantly, integrated documentation for Android framework APIs.  \r\n+ OpenGL ES is an API for programming advanced 3D graphics in embedded devices such as smartphones, consoles, vehicles, etc. It is an OpenGL API modified to meet the needs and constraints of the embedded devices such as limited processing capabilities and memory availability, low memory bandwidth, power consumption factor, and lack of floating-point hardware.  \r\n+ Rajawali is an OpenGL ES 2.0 Based 3D Framework for Android built on top of the OpenGL ES 2.0 API. From its many functionalities I have used .obj file importer, frustrum culling and quaternion based rotations.\r\n+ MeshLab is a system for the processing and editing of unstructured 3D triangular meshes. The system helps to process big, unstructured models by providing a set of tools for editing, cleaning, healing, inspecting, rendering and converting this kind of meshes. I downloaded many open source .obj files from different web sites to load them into my application. Preprocessing those files in MeshLab before using in my application helped to remove duplicated, unreferenced vertices, null faces and small isolated components as well as provided coherent normal unification, flipping, erasing of non manifold faces and automatic filling of holes.  \r\n+ Octave is a high-level programming language, primarily intended for numerical computations. It has adapted gnuplot,a graphing utility for data visualisation. Octave was used in this project to plot graphs.\r\n                                     Chapter 3. Design and implementation   \r\n  **3.1.Loading a model into the application and fitting it into the screen**  \r\n  In my application I’ve used models defined in geometry definition file  - .obj. This file format is a simple data-format that represents essential information about 3D geometry: position of each vertex, the UV position of each texture coordinate vertex, the faces and the normals. These information takes hundreds of lines and looks similar to this:  \r\n_1. v 3.268430 -28.849100 -135.483398 0.752941 0.752941 0.752941_  \r\n_2. vn 1.234372 -4.395962 -4.233196_  \r\n_3. v 4.963750 -28.260300 -135.839600 0.752941 0.752941 0.752941_  \r\n_4. vn 1.893831 -3.827220 -4.498524_  \r\n_..._  \r\n_1178. f 3971//3971 3877//3877 3788//3788_  \r\n_1179. f 2489//2489 2608//2608 2447//2447_  \r\n_1180. f 3686//3686 3472//3472 3473//3473_  \r\n_..._  \r\n  Models are loaded into the application from this file by parsing each line and saving the information into the Arraylists or a similar data structure of vertices, normals, texture coordinates, colours and indices. This information after being sorted into lists, is now ready to be drawn using typical Opengl ES program. For this purpose I’ve used Rajawali, an open source 3D framework for Android, by Dennis Ippel. It has a file parsing functionality  which can import geometry definition file formats such as .obj, .md2, .3ds and .fbx files and shader rendering class using Opengl ES, which is precisely what is needed.  \r\n  Models come in different sizes and are located in different coordinates in camera space. When loaded it would be nice if a model is automatically placed in the centre of the screen and fit it, instead of manually changing camera position or far and near planes or other settings for every model.  \r\n  To position a model object to the centre of the screen, it has to be placed to the centre of the bounding box. To deal with bounding box I’ve used Rajawali’s BoundingBox class. The centre of the box can be found by finding midpoint of maximum and minimum points of the bounding box. Once this point is found, the model’s position can be simply reset.  \r\n                `Geometry3D anObject = mObjectGroup.getGeometry();`  \r\n                `BoundingBox aBox = anObject.getBoundingBox();`  \r\n                `Number3D min = aBox.getMin();`\r\n                `Number3D max = aBox.getMax();`\r\n                `float averageX = (min.x+max.x)/2;`\r\n                `float averageY = (min.y+max.y)/2;`\r\n\t\t`float averageZ = (min.z+max.z)/2;`\r\n                `mObjectGroup.setPosition(-averageX, -averageY, -averageZ);`  \r\n  To fit the model into the screen it has to be scaled to fit into unit bounding box. For that, the scale factor, i.e. how much the model should be scaled, needs to be calculated. The amount, by which unit bounding box of the model is bigger than the bounding box of the model, is the scaling factor. The depth, width and height of the unit box are all equal to 2(|-1|+1).\r\n`float bbWidthx = max.x – min.x;`\r\n`float bbHeighty = max.y - min.y;`\r\n`float bbDepthz = max.z - min.z;`\r\n`float scaleFactorx = 2/bbWidthx;`\r\n`float scaleFactory = 2/bbHeighty;`\r\n`float scaleFactorz = 2/bbDepthz;`\r\n`float maxScale = Math.min(Math.min(scaleFactory,scaleFactorz),scaleFactorx);`\r\n`mObjectGroup.setScale(maxScale,maxScale,maxScale);` \r\n\r\n ","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}